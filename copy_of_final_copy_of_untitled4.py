# -*- coding: utf-8 -*-
"""Copy of Final_Copy_of_Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1okCVVzHL8Ej1tqDX2EOBmSpRfT6wegrZ
"""

# prompt: read df_cleaned_with_dummies.xlsx

import pandas as pd

# Assuming df_cleaned_with_dummies.xlsx is in the same directory as your notebook
try:
  df = pd.read_excel('df_cleaned_with_dummies.xlsx')
  print("Successfully loaded df_cleaned_with_dummies.xlsx")
  print(df.head()) # Display the first few rows to confirm
except FileNotFoundError:
  print("Error: df_cleaned_with_dummies.xlsx not found.")
  print("Please make sure the file is in the correct directory or provide the full path.")
except Exception as e:
  print(f"An error occurred: {e}")

!pip install plotly

df.columns

import pandas as pd

# 1. SÃ©lection des colonnes Lean
lean_columns = df.filter(regex=r'^Lean')

# 2. Calcul du nombre d'entreprises utilisant chaque mÃ©thode Lean
lean_table = lean_columns.sum().to_frame(name="Nombre d'entreprises")

# 3. Ajouter le pourcentage d'utilisation (optionnel mais souvent utile)
lean_table["Pourcentage (%)"] = (lean_table["Nombre d'entreprises"] / len(df)) * 100

# 4. Trier par niveau d'utilisation (du plus utilisÃ© au moins utilisÃ©)
lean_table = lean_table.sort_values(by="Nombre d'entreprises", ascending=False)

# 5. Affichage du tableau complet
lean_table

import pandas as pd

# 1. SÃ©lection des colonnes Industrie 4.0
tech_columns = df.filter(regex=r'^Tech_')

# 2. Calcul du nombre d'entreprises utilisant chaque technologie
tech_table = tech_columns.sum().to_frame(name="Nombre d'entreprises")

# 3. Ajouter le pourcentage d'utilisation
tech_table["Pourcentage (%)"] = (tech_table["Nombre d'entreprises"] / len(df)) * 100

# 4. Trier par niveau d'utilisation (du plus utilisÃ© au moins utilisÃ©)
tech_table = tech_table.sort_values(by="Nombre d'entreprises", ascending=False)

# 5. Affichage du tableau complet
tech_table

# prompt: Genrate the data description in a clear table

from tabulate import tabulate

# Select the relevant columns for description
df_description = df.describe()

# Reset index to turn the index (like count, mean, std) into a column for tabulate
df_description = df_description.reset_index()

# Rename the index column for clarity
df_description = df_description.rename(columns={'index': 'Description Metric'})

# Print the description in a table format
print(tabulate(df_description, headers='keys', tablefmt='psql'))

# General info
df.info()

# Statistical summary
df.describe()

# Check for missing values
df.isnull().sum()

# Check for duplicate rows
df.duplicated().sum()

# prompt: distribution des values de 'Secteur industriel'

distribution_secteur = df['Secteur industriel'].value_counts()
print("\nDistribution des valeurs dans 'Secteur industriel':")
print(tabulate(distribution_secteur.reset_index(), headers=['Secteur industriel', 'Count'], tablefmt='psql'))

# prompt: distribution des values de 'Secteur industriel'

import matplotlib.pyplot as plt
import seaborn as sns # Import seaborn

# Get the value counts for the specified column
sector_distribution = df['Secteur industriel'].value_counts()

print("Distribution des valeurs pour 'Quelle est le secteur de votre entreprise ? ':")
print(sector_distribution)

# Optional: Visualize the distribution
plt.figure(figsize=(12, 7))
sns.countplot(data=df, y='Secteur industriel', order=sector_distribution.index)
plt.title("Distribution des secteurs d'entreprise")
plt.xlabel("Count")
plt.ylabel("Secteur Industriel")
plt.tight_layout()
plt.show()

# prompt: i want to code taill entreprise >500 ; Grande entreprise and other columns : PME

# Assuming the column 'Taille entreprise ' exists in your DataFrame 'df'
if 'df' in locals() and not df.empty and 'Taille entreprise ' in df.columns:
  # Create a new column 'taille_categorie'
  # Map '>500 ' and 'Grande entreprise' to 'Grande entreprise'
  # Map all other values to 'PME'
  df['taille_categorie'] = df['Taille entreprise '].apply(
      lambda x: 'GE' if x in ['>= 500', 'GE'] else 'PME'
  )

  # Display the first few rows with the new column to verify
  print("\nDataFrame with new 'taille_categorie' column:")
  print(df[['Taille entreprise ', 'taille_categorie']].head())

  # Display value counts for the new column
  print("\nValue counts for 'taille_categorie':")
  print(df['taille_categorie'].value_counts())

else:
  print("DataFrame 'df' is not loaded, is empty, or 'Taille entreprise ' column not found.")

colonnes = [
    "Leadership - Engagement Lean ",
    "Leadership - Engagement DT",
    "Leadership - StratÃ©gie ",
    "Leadership - Communication",
    "Supply Chain - Collaboration inter-organisationnelle",
    "Supply Chain - TraÃ§abilitÃ©",
    "Supply Chain - Impact sur les employÃ©es",
    "OpÃ©rations - Standardisation des processus",
    "OpÃ©rations - Juste-Ã -temps (JAT)",
    "OpÃ©rations - Gestion des rÃ©sistances",
    "Technologies - ConnectivitÃ© et gestion des donnÃ©es",
    "Technologies - Automatisation",
    "Technologies - Pilotage du changement",
    "Organisation apprenante  - Formation et dÃ©veloppement des compÃ©tences",
    "Organisation apprenante  - Collaboration et Partage des Connaissances",
    "Organisation apprenante  - FlexibilitÃ© organisationnelle",
]

import pandas as pd

# DonnÃ©es initiales


# Dictionnaire de mapping
mapping = {
    'Ã©lectrique/Ã©lectronique/Ã©lectromÃ©nager': 'Ã‰lectronique et Ã‰nergie',
    'cÃ¢blage': 'Ã‰lectronique et Ã‰nergie',
    'Energitique': 'Ã‰lectronique et Ã‰nergie',
    'AÃ©ronautique': 'Transport et MobilitÃ©',
    'Automobile': 'Transport et MobilitÃ©',
    'Transport': 'Transport et MobilitÃ©',
    'Agroalimentaire': 'Agro-Pharma',
    'Pharmaceutique': 'Agro-Pharma',
    'mÃ©canique et mÃ©tallurgie': 'Industrie Lourde',
    'textile et habillement': 'Industrie Lourde',
    'Consulting': 'Services et Autres',
    'DÃ©veloppement embarquÃ©': 'Services et Autres',
    'Autres': 'Services et Autres'
}


# Application du mapping
df['Secteur RegroupÃ©'] = df['Secteur industriel'].map(mapping)

print(df)

df.head()

# prompt: distribution du Secteur RegroupÃ©

# Get the value counts for the new grouped column
grouped_sector_distribution = df['Secteur RegroupÃ©'].value_counts()

print("\nDistribution des valeurs pour 'Secteur RegroupÃ©':")
print(tabulate(grouped_sector_distribution.reset_index(), headers=['Secteur RegroupÃ©', 'Count'], tablefmt='psql'))

# Visualize the distribution of the grouped sectors
plt.figure(figsize=(10, 6))
sns.countplot(data=df, y='Secteur RegroupÃ©', order=grouped_sector_distribution.index)
plt.title("Distribution des secteurs industriels regroupÃ©s")
plt.xlabel("Count")
plt.ylabel("Secteur RegroupÃ©")
plt.tight_layout()
plt.show()

# Create 'Lean_Juste Ã  temps' column
df['Lean_Juste Ã  temps'] = df['OpÃ©rations - Juste-Ã -temps (JAT)'].apply(lambda x: 1 if x in [4, 5] else 0)

# Lean methods
df['Lean_MÃ©thode TPM / TRS'] = df['Lean_MÃ©thode TPM / TRS'] | df['Lean_TPM / TRS method']
df.drop(columns=['Lean_TPM / TRS method'], inplace=True)

df['Lean_DDMRP/ hoshin kanri'] = (
    df['Lean_DDMRP/ hoshin kanri'] |
    df['Lean_DDMRP'] |
    df['Lean_Maki-Gami/Hoshinâ€¦etc']
)
df.drop(columns=['Lean_DDMRP', 'Lean_Maki-Gami/Hoshinâ€¦etc'], inplace=True)

df['Lean_Just in time'] = df['Lean_Juste Ã  temps'] | df['Lean_Just in time']
df.drop(columns=['Lean_Just in time'], inplace=True)

# Tech tools
df['Tech_RÃ©alitÃ© augmentÃ©e'] = df['Tech_RÃ©alitÃ© augmentÃ©e'] | df['Tech_Augmented reality']
df.drop(columns=['Tech_Augmented reality'], inplace=True)

df['Tech_SystÃ¨mes cyber physiques'] = df['Tech_SystÃ¨mes cyber physiques'] | df['Tech_Cyber â€‹â€‹physical systems']
df.drop(columns=['Tech_Cyber â€‹â€‹physical systems'], inplace=True)

df['Tech_Intelligence artificielle'] = df['Tech_Intelligence artificielle'] | df['Tech_Artificial intelligence']
df.drop(columns=['Tech_Artificial intelligence'], inplace=True)

df['Tech_Robots autonomes'] = df['Tech_Robots autonomes'] | df['Tech_Autonomous robots']
df.drop(columns=['Tech_Autonomous robots'], inplace=True)

# prompt: download df sous forme excel

df.to_excel("processed_df.xlsx", index=False)

# prompt: I want to do culstering of features based just on columns in colonnes

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Check if df is loaded and not empty
if 'df' in locals() and not df.empty:
  # Select the features for clustering
  features = df[colonnes]

  # Handle potential missing values if any (e.g., fill with mean, median, or drop rows)
  # For simplicity, we'll drop rows with any NaN values in the selected columns
  features = features.dropna()

  # Standardize the features
  scaler = StandardScaler()
  scaled_features = scaler.fit_transform(features)

  # Determine the optimal number of clusters (e.g., using the elbow method)
  # We'll try a range of k values and calculate the inertia
  inertia = []
  k_range = range(1, 11) # Example range, adjust as needed
  for k in k_range:
      kmeans = KMeans(n_clusters=k, random_state=42, n_init=10) # Add n_init
      kmeans.fit(scaled_features)
      inertia.append(kmeans.inertia_)

  # Plot the elbow method graph
  plt.figure(figsize=(8, 4))
  plt.plot(k_range, inertia, marker='o')
  plt.xlabel('Number of clusters (k)')
  plt.ylabel('Inertia')
  plt.title('Elbow Method for Optimal k')
  plt.xticks(k_range)
  plt.grid(True)
  plt.show()

# prompt: show silhoette method

from sklearn.metrics import silhouette_score

# Calculate silhouette scores for different numbers of clusters
silhouette_scores = []
# Ensure the range for silhouette score calculation is appropriate (typically k > 1)
k_range_silhouette = range(2, 11) # Example range, adjust as needed

if 'scaled_features' in locals() and scaled_features.shape[0] > 1:
  for k in k_range_silhouette:
      kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
      kmeans.fit(scaled_features)
      score = silhouette_score(scaled_features, kmeans.labels_)
      silhouette_scores.append(score)
      print(f"Silhouette score for k={k}: {score}")

  # Plot the silhouette scores graph
  plt.figure(figsize=(8, 4))
  plt.plot(k_range_silhouette, silhouette_scores, marker='o')
  plt.xlabel('Number of clusters (k)')
  plt.ylabel('Silhouette Score')
  plt.title('Silhouette Method for Optimal k')
  plt.xticks(k_range_silhouette)
  plt.grid(True)
  plt.show()
else:
  print("Scaled features not available or has insufficient data points for silhouette score calculation.")

"""
import plotly.express as px

# --- Calcul des moyennes par dimension ---
def moyenne_dimension(dimensions):
    return df_scores[df_scores["Dimension"].isin(dimensions)]["Score"].mean()

x_dim = ["Technologies", "OpÃ©rations"]
y_dim = ["StratÃ©gie - Leadership", "Supply Chain"]
horizontal_dim = ["Organisation Apprenante"]

x_value = moyenne_dimension(x_dim)
y_value = moyenne_dimension(y_dim)
size_value = moyenne_dimension(horizontal_dim)

fig = px.scatter(
    x=[x_value],
    y=[y_value],
    size=[size_value],
    color=[size_value],
    text=["Entreprise analysÃ©e"],
    labels={"x": "Axe Technico-opÃ©rationnel", "y": "Axe Engagement StratÃ©gique"},
    color_continuous_scale="Blues",
    size_max=60
)

fig.update_traces(marker=dict(line=dict(width=2, color='DarkSlateGrey')), textposition='top center')
fig.update_layout(
    title="ðŸ§­ Positionnement de lâ€™entreprise sur les axes de maturitÃ© Lean 4.0",
    xaxis_title="Axe Technico-opÃ©rationnel (Technologies & OpÃ©rations)",
    yaxis_title="Axe Engagement StratÃ©gique (Leadership & Supply Chain)",
    xaxis=dict(range=[1, 5]),
    yaxis=dict(range=[1, 5]),
    height=600
)

st.plotly_chart(fig, use_container_width=True)
"""

# Based on the elbow plot, choose an appropriate number of clusters (let's say k=4 as an example)
  optimal_k = 3 # Replace with the value you choose from the plot

  # Perform KMeans clustering with the chosen number of clusters
  kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10) # Add n_init
  df['cluster'] = kmeans.fit_predict(scaled_features)

  # Analyze the resulting clusters
  print("\nCluster sizes:")
  print(df['cluster'].value_counts())

  print("\nMean values of features per cluster:")
  print(df.groupby('cluster')[colonnes].mean())

  # Optional: Visualize the clusters (e.g., using PCA for dimensionality reduction)
  from sklearn.decomposition import PCA

  if scaled_features.shape[1] > 2: # Only perform PCA if more than 2 features
    pca = PCA(n_components=2)
    principal_components = pca.fit_transform(scaled_features)
    pca_df = pd.DataFrame(data = principal_components, columns = ['principal component 1', 'principal component 2'])
    pca_df['cluster'] = df['cluster'].reset_index(drop=True) # Add cluster labels to the PCA dataframe

    plt.figure(figsize=(8,6))
    for cluster_label in sorted(pca_df['cluster'].unique()):
        subset = pca_df[pca_df['cluster'] == cluster_label]
        plt.scatter(subset['principal component 1'], subset['principal component 2'], label=f'Cluster {cluster_label}', alpha=0.7)
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.title('2D PCA of Clustered Features')
    plt.legend()
    plt.grid(True)
    plt.show()
  else:
      print("\nNot enough features to perform PCA for visualization.")

import pandas as pd
import numpy as np

# Supposons que pca est dÃ©jÃ  entraÃ®nÃ© et que features est la DataFrame des features d'origine
components = pca.components_
feature_names = features.columns

# CrÃ©er un DataFrame des composantes, valeurs absolues
abs_components = np.abs(components)

# Calculer la somme des poids par composante (ligne)
sums = abs_components.sum(axis=1).reshape(-1, 1)

# Pourcentage de contribution par feature dans chaque composante
pct_contrib = abs_components / sums * 100

# DataFrame clair avec les contributions en pourcentage arrondies Ã  2 dÃ©cimales
df_contrib = pd.DataFrame(pct_contrib, columns=feature_names,
                          index=[f'PC{i+1}' for i in range(pca.n_components_)])

# Trier chaque ligne du DataFrame par ordre dÃ©croissant et afficher
for pc in df_contrib.index:
    print(f"\nTop contributions for {pc}:")
    display(df_contrib.loc[pc].sort_values(ascending=False).to_frame().rename(columns={pc: 'Contribution (%)'}))

# Extraire les dimensions depuis les noms de colonnes
dimensions = features.columns.str.extract(r'^(.*?)\s*-\s*')[0]

# CrÃ©er un mapping : sous-dimension âž dimension
feature_to_dimension = dict(zip(features.columns, dimensions))

# Remplacer les colonnes de df_contrib par leur dimension
df_contrib_grouped = df_contrib.copy()
df_contrib_grouped.columns = [feature_to_dimension[col] for col in df_contrib.columns]

# Grouper les contributions par dimension (somme des sous-dimensions)
df_contrib_by_dimension = df_contrib_grouped.groupby(axis=1, level=0).sum()

# Affichage triÃ© pour chaque composante principale
for pc in df_contrib_by_dimension.index:
    print(f"\nTotal contribution by dimension for {pc}:")
    display(df_contrib_by_dimension.loc[pc].sort_values(ascending=False).to_frame().rename(columns={pc: 'Total Contribution (%)'}))

import pandas as pd

# Assuming 'features' (for initial dimension extraction logic) and 'df_contrib' (containing feature contributions to PCs)
# are available from previous steps. 'df_contrib' has original individual feature contributions.

# 1. Create a copy of df_contrib to modify
df_contrib_temp = df_contrib.copy()

# 2. Identify the specific 'Leadership' engagement columns to be averaged.
leadership_engagement_cols_to_average = [
    "Leadership - Engagement Lean ",
    "Leadership - Engagement DT"
]

# 3. Calculate the average contribution for these specific columns and replace them.
if all(col in df_contrib_temp.columns for col in leadership_engagement_cols_to_average):
    # Calculate the average contribution across these two for each PC (row)
    df_contrib_temp['Leadership - Engagement (Avg)'] = df_contrib_temp[leadership_engagement_cols_to_average].mean(axis=1)
    # Drop the original two columns from the temporary DataFrame
    df_contrib_temp = df_contrib_temp.drop(columns=leadership_engagement_cols_to_average)
else:
    print(f"Warning: One or more of {leadership_engagement_cols_to_average} not found in df_contrib columns. Proceeding without specific averaging for these columns.")

# 4. Re-extract dimensions and create a mapping using the *modified* column names from df_contrib_temp.
# This ensures the new 'Leadership - Engagement (Avg)' column is correctly mapped to 'Leadership'.
dimensions_for_mapping = pd.Series(df_contrib_temp.columns).str.extract(r'^(.*?)\s*-\s*')[0]
feature_to_dimension = dict(zip(df_contrib_temp.columns, dimensions_for_mapping))

# Handle potential NaN from extract if a column has no '-' (e.g., if 'Cluster' was in df_contrib_temp accidentally)
# or for the new 'Leadership - Engagement (Avg)' column if it doesn't match the regex pattern.
# For 'Leadership - Engagement (Avg)', we explicitly ensure it maps to 'Leadership'.
if 'Leadership - Engagement (Avg)' in feature_to_dimension:
    feature_to_dimension['Leadership - Engagement (Avg)'] = 'Leadership'

# 5. Rename the columns of df_contrib_temp to their higher-level dimensions
# This step will create multiple columns with the same dimension name if there are multiple sub-features.
df_contrib_grouped = df_contrib_temp.rename(columns=feature_to_dimension)

# 6. Group the contributions by dimension name, summing them up.
# This aggregates all sub-feature contributions under their respective dimensions.
# The `groupby(axis=1, level=0).sum()` effectively sums columns with identical names.
df_contrib_by_dimension = df_contrib_grouped.groupby(level=0, axis=1).sum()

# Affichage triÃ© pour chaque composante principale
for pc in df_contrib_by_dimension.index:
    print(f"\nTotal contribution by dimension for {pc} (Leadership Engagement contributions averaged):")
    display(df_contrib_by_dimension.loc[pc].sort_values(ascending=False).to_frame().rename(columns={pc: 'Total Contribution (%)'}))

import plotly.express as px
import pandas as pd

# Get the feature names used for PCA (these are the 'colonnes' that were used for PCA)
feature_names_for_pca = colonnes

# Get the principal components (loadings)
# We only want the first two components for PC1 and PC2 visualization
loadings = pca.components_.T[:, :2]  # Transpose and take only the first two columns

# Create a DataFrame for loadings
loadings_df = pd.DataFrame(loadings, columns=['PC1_Loading', 'PC2_Loading'], index=feature_names_for_pca)
loadings_df['Feature Name'] = loadings_df.index # Add feature names as a column for hover

# Create a base mapping from sub-dimensions to main dimensions from the 'colonnes' list
dimension_mapping_for_all_features = {sd: sd.split(' - ')[0].strip() for sd in colonnes}

# Assign the 'Dimension' column to loadings_df using the comprehensive mapping
# Only map the features that were actually part of the PCA
loadings_df['Dimension'] = loadings_df.index.map(lambda x: dimension_mapping_for_all_features.get(x, 'Other')) # 'Other' as a fallback

# Create an interactive scatter plot of loadings, now colored by Dimension
fig_loadings = px.scatter(loadings_df, x='PC1_Loading', y='PC2_Loading',
                          color='Dimension', # Color by main dimension
                          text=loadings_df.index,  # Display feature names as text
                          title='Feature Loadings on PC1 and PC2 by Dimension',
                          labels={'PC1_Loading': 'PC1 Loading', 'PC2_Loading': 'PC2 Loading'},
                          hover_name='Feature Name') # Add hover_name for better readability

# Add arrows or lines from the origin to each loading point to emphasize direction and magnitude
for i, row in loadings_df.iterrows():
    fig_loadings.add_shape(type='line',
                           x0=0, y0=0, x1=row['PC1_Loading'], y1=row['PC2_Loading'],
                           line=dict(color='gray', width=1),
                           layer='below') # Draw lines below the points

fig_loadings.update_traces(textposition='top center')
fig_loadings.show()

import plotly.express as px

# Assuming df_contrib contains the contributions of each feature to PC1 and PC2
# df_contrib has PCs as index and features as columns
# To plot features, we need features as rows and PCs as columns, so we transpose.

# Ensure feature names are clean for display
df_contrib_plot = df_contrib.T.reset_index()
df_contrib_plot = df_contrib_plot.rename(columns={'index': 'Sub_Dimension'})

fig = px.scatter(df_contrib_plot,
                 x='PC1',
                 y='PC2',
                 text='Sub_Dimension',
                 size_max=60,
                 title='Sub-dimensions Distribution based on PC1 and PC2 Contributions')

fig.update_traces(textposition='top center')
fig.update_layout(height=700, width=1000)
fig.show()

import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt


# 2. Apply PCA
pca = PCA()
pca.fit(scaled_features)
# 3. Explained variance ratio
explained_var_ratio = pca.explained_variance_ratio_ * 100  # in %

# 4. Plot Scree Plot (bar chart of all components)
plt.figure(figsize=(12, 6))
components = np.arange(1, len(explained_var_ratio) + 1)

bars = plt.bar(components, explained_var_ratio, color='skyblue', edgecolor='black')
plt.xlabel('Composante principale')
plt.ylabel('Variance expliquÃ©e (%)')
plt.title('Scree Plot - Variance expliquÃ©e par chaque composante principale')
plt.xticks(components)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# ðŸ’¬ Ajouter les Ã©tiquettes % sur les barres
for bar, var in zip(bars, explained_var_ratio):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width() / 2, height + 0.5, f'{var:.1f}%',
             ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.show()

# prompt: heatmap of columns dans colonnes
import seaborn as sns # Import seaborn
# Assuming 'colonnes' is the list of column names you want to include in the heatmap
# Calculate the correlation matrix for the selected columns
correlation_matrix = df[colonnes].corr()

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(14, 10)) # Adjust figure size as needed
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Heatmap of Correlation Matrix for Selected Columns')
plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.tight_layout() # Adjust layout to prevent labels from overlapping
plt.show()

# prompt: heatmap of columns in colonnes plus la colonne 'Cluster'

# Combine the selected columns and the 'cluster' column for the heatmap
columns_for_heatmap = colonnes + ['Cluster']

# Calculate the correlation matrix including the 'cluster' column
correlation_matrix_with_cluster = df[columns_for_heatmap].corr()

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(16, 12)) # Adjust figure size as needed to accommodate the new column
sns.heatmap(correlation_matrix_with_cluster, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Heatmap of Correlation Matrix for Selected Columns and Cluster')
plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.tight_layout() # Adjust layout to prevent labels from overlapping
plt.show()

# prompt: Merci d'afficher les colonnes par deux selon les coefficients des correlations les plus grands

# Get the absolute value of the correlation matrix
abs_corr_matrix = correlation_matrix.abs()

# Stack the matrix to get pairs and their correlations, excluding the diagonal
stacked_corr = abs_corr_matrix.stack()
stacked_corr = stacked_corr[stacked_corr.index.get_level_values(0) != stacked_corr.index.get_level_values(1)]

# Sort by absolute correlation in descending order
sorted_corr = stacked_corr.sort_values(ascending=False)

print("\nTop correlated pairs of columns (absolute value):")
# Iterate through the sorted correlations and print pairs without repetition
printed_pairs = set()
for (col1, col2), corr_value in sorted_corr.items():
    # Create a unique key for the pair regardless of order
    pair_key = tuple(sorted((col1, col2)))
    if pair_key not in printed_pairs:
        print(f"'{col1}' and '{col2}': {corr_value:.2f}")
        printed_pairs.add(pair_key)

# prompt: Merci d'afficher les colonnes par deux selon les coefficients des correlations les plus grands entre 'Cluster' et les colonnes de la variable colonnes

# Calculate the absolute correlation between 'Cluster' and each of the other columns
cluster_correlations = df[colonnes + ['Cluster']].corr()['Cluster'].abs().drop('Cluster')

# Sort the correlations in descending order
sorted_cluster_correlations = cluster_correlations.sort_values(ascending=False)

print("\nTop correlations between 'Cluster' and other columns (absolute value):")

# Iterate through the sorted correlations and print column name and correlation
for col, corr_value in sorted_cluster_correlations.items():
    print(f"'{col}': {corr_value:.2f}")

df.columns

# Create 'Lean_Juste Ã  temps' column
df['Lean_Juste Ã  temps'] = df['OpÃ©rations - Juste-Ã -temps (JAT)'].apply(lambda x: 1 if x in [4, 5] else 0)
#df['Tech_IoT'] = df['Technologies - ConnectivitÃ© et gestion des donnÃ©es'].apply(lambda x: 1 if x in [4, 5] else 0)

'''
# Lean methods
df['Lean_MÃ©thode TPM / TRS'] = df['Lean_MÃ©thode TPM / TRS'] | df['Lean_TPM / TRS method']
df.drop(columns=['Lean_TPM / TRS method'], inplace=True)

df['Lean_DDMRP/ hoshin kanri'] = (
    df['Lean_DDMRP/ hoshin kanri'] |
    df['Lean_DDMRP'] |
    df['Lean_Maki-Gami/Hoshinâ€¦etc']
)
df.drop(columns=['Lean_DDMRP', 'Lean_Maki-Gami/Hoshinâ€¦etc'], inplace=True)

df['Lean_Just in time'] = df['Lean_Juste Ã  temps'] | df['Lean_Just in time']
df.drop(columns=['Lean_Just in time'], inplace=True)

# Tech tools
df['Tech_RÃ©alitÃ© augmentÃ©e'] = df['Tech_RÃ©alitÃ© augmentÃ©e'] | df['Tech_Augmented reality']
df.drop(columns=['Tech_Augmented reality'], inplace=True)

df['Tech_SystÃ¨mes cyber physiques'] = df['Tech_SystÃ¨mes cyber physiques'] | df['Tech_Cyber â€‹â€‹physical systems']
df.drop(columns=['Tech_Cyber â€‹â€‹physical systems'], inplace=True)

df['Tech_Intelligence artificielle'] = df['Tech_Intelligence artificielle'] | df['Tech_Artificial intelligence']
df.drop(columns=['Tech_Artificial intelligence'], inplace=True)

df['Tech_Robots autonomes'] = df['Tech_Robots autonomes'] | df['Tech_Autonomous robots']
df.drop(columns=['Tech_Autonomous robots'], inplace=True)
'''

# prompt: i want to code taill entreprise >500 ; Grande entreprise and other columns : PME

# Assuming the column 'Taille entreprise ' exists in your DataFrame 'df'
if 'df' in locals() and not df.empty and 'Taille entreprise ' in df.columns:
  # Create a new column 'taille_categorie'
  # Map '>500 ' and 'Grande entreprise' to 'Grande entreprise'
  # Map all other values to 'PME'
  df['taille_categorie'] = df['Taille entreprise '].apply(
      lambda x: 'GE' if x in ['>= 500', 'GE'] else 'PME'
  )

  # Display the first few rows with the new column to verify
  print("\nDataFrame with new 'taille_categorie' column:")
  print(df[['Taille entreprise ', 'taille_categorie']].head())

  # Display value counts for the new column
  print("\nValue counts for 'taille_categorie':")
  print(df['taille_categorie'].value_counts())

else:
  print("DataFrame 'df' is not loaded, is empty, or 'Taille entreprise ' column not found.")

# prompt: Appliquer dummies to 'taille_categorie'

# Assuming 'taille_categorie' is a categorical column
# Apply one-hot encoding to 'taille_categorie'
df = pd.get_dummies(df, columns=['taille_categorie'], prefix='tailleGR')

# Display the first few rows to see the new dummy columns
print("\nDataFrame after applying one-hot encoding to 'taille_categorie':")
print(df.head())

# prompt: Appliquer dummies to 'Secteur RegroupÃ©'

# Apply one-hot encoding to 'Secteur RegroupÃ©'
df = pd.get_dummies(df, columns=['Secteur RegroupÃ©'], prefix='SecteurREG')

# Display the first few rows to see the new dummy columns
print("\nDataFrame after applying one-hot encoding to 'Secteur RegroupÃ©':")
print(df.head())

# prompt: Ajouter une variable qui regroupe les colonnes dummies du secteur regroupÃ©

# Variable to group the dummy columns for 'Secteur RegroupÃ©'
secteur_dummies = [col for col in df.columns if col.startswith('SecteurREG_')]

# prompt: Fixer le types des variables dans colonnes Ã  integer

# Attempt to convert specified columns to integer type
for col in colonnes:
  if col in df.columns:
    try:
      # Convert to numeric first, coercing errors, then to Int64 (supports NaNs)
      df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')
      print(f"Successfully converted column '{col}' to Int64.")
    except Exception as e:
      print(f"Could not convert column '{col}' to Int64: {e}")
  else:
    print(f"Column '{col}' not found in DataFrame.")

# Display the data types to confirm changes
print("\nData types after conversion attempt:")
print(df[colonnes].info())

# prompt: heatmap of columns in colonnes plus la colonne 'Cluster'

# Combine the selected columns and the 'cluster' column for the heatmap
columns_for_heatmap = colonnes + secteur_dummies + ['tailleGR_GE'] + ['tailleGR_PME'] +['Cluster']

# Calculate the correlation matrix including the 'cluster' column
correlation_matrix_with_cluster = df[columns_for_heatmap].corr()

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(16, 12)) # Adjust figure size as needed to accommodate the new column
sns.heatmap(correlation_matrix_with_cluster, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Heatmap of Correlation Matrix for Selected Columns and Cluster')
plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.tight_layout() # Adjust layout to prevent labels from overlapping
plt.show()

# Liste des colonnes Lean et Tech spÃ©cifiques que tu veux garder
lean_cols = [
    'Lean_5S',
    'Lean_Value Stream Mapping (VSM)',
    'Lean_Kaizen',
    'Lean_MÃ©thode TPM / TRS',
    'Lean_QRQC',
    'Lean_Kanban',
    'Lean_Takt Time',
    'Lean_6 sigma',
    'Lean_Heijunka',
    'Lean_Juste Ã  temps',
    'Lean_Poka Yoke',
]

tech_cols = [
    'Tech_ERP (Enterprise Resource Planning)',
    'Tech_MES (Manufacturing Execution System)',
    'Tech_WMS (Warehouse Management System)',
    'Tech_SystÃ¨mes cyber physiques',
    'Tech_RFID',
    'Tech_Big Data et Analytics',
    'Tech_Fabrication additive (Impression 3D)',
    'Tech_Robots autonomes',
    'Tech_Simulation',
    'Tech_RÃ©alitÃ© augmentÃ©e',
    'Tech_Cloud computing',
    'Tech_Intelligence artificielle'
]

# Regrouper les colonnes cibles
target_cols = lean_cols + tech_cols

# Filtrer celles qui existent rÃ©ellement dans le DataFrame
dummy_cols_filtered = df[[col for col in target_cols if col in df.columns]]

# prompt: heatmap of columns in colonnes plus la colonne 'Cluster'

# Combine the selected columns and the 'cluster' column for the heatmap
# Extraire les noms des colonnes et ajouter 'Cluster'
columns_for_heatmap = list(dummy_cols_filtered.columns) + ['Cluster']


# Calculate the correlation matrix including the 'cluster' column
correlation_matrix_with_cluster = df[columns_for_heatmap].corr()

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(16, 12)) # Adjust figure size as needed to accommodate the new column
sns.heatmap(correlation_matrix_with_cluster, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Heatmap of Correlation Matrix for Selected Columns and Cluster')
plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.tight_layout() # Adjust layout to prevent labels from overlapping
plt.show()

# prompt: Merci d'afficher les colonnes des mÃ©thodes Lean par pairs selon la force de corrÃ©lation   entre les mÃ©thodes Lean

# Select only the Lean method columns
lean_method_cols = [col for col in df.columns if col.startswith('Lean_')]

# Calculate the correlation matrix for Lean method columns
correlation_matrix_lean = df[lean_method_cols].corr()

# Get the absolute value of the correlation matrix
abs_corr_matrix_lean = correlation_matrix_lean.abs()

# Stack the matrix to get pairs and their correlations, excluding the diagonal
stacked_corr_lean = abs_corr_matrix_lean.stack()
stacked_corr_lean = stacked_corr_lean[stacked_corr_lean.index.get_level_values(0) != stacked_corr_lean.index.get_level_values(1)]

# Sort by absolute correlation in descending order
sorted_corr_lean = stacked_corr_lean.sort_values(ascending=False)

print("\nCorrÃ©lation entre les mÃ©thodes Lean (par paires, triÃ©e par force de corrÃ©lation):")

# Iterate through the sorted correlations and print pairs without repetition
printed_lean_pairs = set()
for (col1, col2), corr_value in sorted_corr_lean.items():
    # Create a unique key for the pair regardless of order
    pair_key = tuple(sorted((col1, col2)))
    if pair_key not in printed_lean_pairs:
        print(f"'{col1}' et '{col2}': {corr_value:.2f}")
        printed_lean_pairs.add(pair_key)

# prompt: Merci d'afficher les colonnes des mÃ©thodes Lean par pairs selon la force de corrÃ©lation   entre les technologies industrie 4.0

# Select only the Tech columns
tech_cols_for_corr = [col for col in df.columns if col.startswith('Tech_')]

# Calculate the correlation matrix for Tech columns
correlation_matrix_tech = df[tech_cols_for_corr].corr()

# Get the absolute value of the correlation matrix
abs_corr_matrix_tech = correlation_matrix_tech.abs()

# Stack the matrix to get pairs and their correlations, excluding the diagonal
stacked_corr_tech = abs_corr_matrix_tech.stack()
stacked_corr_tech = stacked_corr_tech[stacked_corr_tech.index.get_level_values(0) != stacked_corr_tech.index.get_level_values(1)]

# Sort by absolute correlation in descending order
sorted_corr_tech = stacked_corr_tech.sort_values(ascending=False)

print("\nCorrÃ©lation entre les technologies Industrie 4.0 (par paires, triÃ©e par force de corrÃ©lation):")

# Iterate through the sorted correlations and print pairs without repetition
printed_tech_pairs = set()
for (col1, col2), corr_value in sorted_corr_tech.items():
    # Create a unique key for the pair regardless of order
    pair_key = tuple(sorted((col1, col2)))
    if pair_key not in printed_tech_pairs:
        print(f"'{col1}' et '{col2}': {corr_value:.2f}")
        printed_tech_pairs.add(pair_key)

# prompt: Merci d'afficher les colonnes des mÃ©thodes Lean et technologies industrie 4.0 par pairs selon la force de corrÃ©lation

# Select the Lean method columns and Tech columns for cross-correlation
lean_method_cols_filtered = [col for col in lean_cols if col in df.columns]
tech_cols_filtered = [col for col in tech_cols if col in df.columns]

# Calculate the correlation matrix between Lean method columns and Tech columns
# We select subsets of the dataframe and calculate their correlation
correlation_matrix_lean_tech = df[lean_method_cols_filtered].corrwith(df[tech_cols_filtered])

# The result is a Series where the index is a Lean column and the value is
# the correlation with the Tech column at the corresponding index in the tech_cols_filtered list.
# This is not exactly the pairwise correlation matrix we need between ALL Lean and ALL Tech.

# To get the pairwise correlation between ALL Lean and ALL Tech columns:
# Select the full dataframe with only the Lean and Tech columns of interest
df_lean_tech = df[lean_method_cols_filtered + tech_cols_filtered]

# Calculate the full correlation matrix for this subset
full_correlation_matrix = df_lean_tech.corr()

# Extract the sub-matrix representing the correlation between Lean and Tech columns
# The rows will be Lean columns, and the columns will be Tech columns
correlation_lean_vs_tech = full_correlation_matrix.loc[lean_method_cols_filtered, tech_cols_filtered]

# Get the absolute value of the correlation matrix
abs_corr_lean_vs_tech = correlation_lean_vs_tech.abs()

# Stack the matrix to get pairs (Lean_col, Tech_col) and their correlations
stacked_corr_lean_tech = abs_corr_lean_vs_tech.stack()

# Sort by absolute correlation in descending order
sorted_corr_lean_tech = stacked_corr_lean_tech.sort_values(ascending=False)

print("\nCorrÃ©lation entre les mÃ©thodes Lean et les technologies Industrie 4.0 (par paires, triÃ©e par force de corrÃ©lation):")

# Iterate through the sorted correlations and print pairs
for (lean_col, tech_col), corr_value in sorted_corr_lean_tech.items():
    print(f"'{lean_col}' et '{tech_col}': {corr_value:.2f}")



# prompt: diplsay first 5 rows of all columns

print("\nFirst 5 rows of all columns:")
print(df.head().to_markdown(index=False))

dummy_cols = [col for col in df.columns if col.startswith('Lean_') or col.startswith('Tech_')]

df[colonnes] = df[colonnes].astype('float')

# prompt: === ðŸ“ˆ Visualizing Average Survey Scores per Cluster ===
# === ðŸ“ˆ Visualizing Average Lean Method and Tech Presence per Cluster ===
# Show heatmaps USE BLUE AND GREEN COLORS

import seaborn as sns

# Calculate the average survey scores per cluster
avg_scores_per_cluster = df.groupby('Cluster')[colonnes].mean()

# Visualize the average survey scores per cluster using a heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(avg_scores_per_cluster.T, cmap="YlGnBu", annot=True, fmt=".2f")
plt.title('Average Survey Scores per Cluster')
plt.xlabel('Cluster')
plt.ylabel('Survey Questions')
plt.show()


# Calculate the average Lean method and Tech presence per cluster
avg_lean_tech_per_cluster = df.groupby('cluster')[dummy_cols].mean()

# Visualize the average Lean method and Tech presence per cluster using a heatmap
plt.figure(figsize=(16, 10))
sns.heatmap(avg_lean_tech_per_cluster.T, annot=True, fmt=".2f", cmap="Blues")
plt.title('Average Presence (Proportion) of Lean Methods and Techs per Cluster')
plt.xlabel('Cluster')
plt.ylabel('Method/Technology')
plt.tight_layout()
plt.show()

# 1. Calculer les moyennes des sous-dimensions par cluster
cluster_means = df.groupby('cluster')[colonnes].mean()

# 2. Calcul de la moyenne globale (moyenne des moyennes)
global_means = cluster_means.mean(axis=0)

# 3. Calcul de la variance inter-cluster pour chaque feature
inter_cluster_variance = ((cluster_means - global_means) ** 2).mean(axis=0)

# 4. Normalisation en poids (%) pour interprÃ©tation
feature_importance = (inter_cluster_variance / inter_cluster_variance.sum()) * 100

# 5. Affichage triÃ© des poids par ordre dÃ©croissant
print("\nPoids des sous-dimensions (importance relative dans le KMeans):")
print(feature_importance.sort_values(ascending=False).round(2))

# 1. Moyennes des sous-dimensions (features) par cluster
cluster_means = df.groupby('cluster')[colonnes].mean()

# 2. Moyenne globale de chaque feature
global_means = cluster_means.mean()

# 3. Variance inter-cluster : Ã©cart des clusters par rapport Ã  la moyenne globale
inter_cluster_var = ((cluster_means - global_means) ** 2).mean(axis=0)

# 4. Normalisation pour obtenir les poids (somme = 1)
weights = inter_cluster_var * 100/ inter_cluster_var.sum()

# 5. Affichage des poids triÃ©s
weights_sorted = weights.sort_values(ascending=False).round(4)
print("ðŸ“Š Poids normalisÃ©s des sous-dimensions (importance relative dans le KMeans) :\n")
print(weights_sorted)

df.head()

# prompt: afficher les 5 premiers lignes avec toutes s les colonnes

print(df.head().to_markdown(index=False))

# prompt: cammabÃ¨re de colonne 'taille_categorie'

# Check if the 'taille_categorie' column exists
if 'taille_categorie' in df.columns:
    # Get the value counts for the 'taille_categorie' column
    taille_categorie_counts = df['taille_categorie'].value_counts()

    # Create the pie chart
    plt.figure(figsize=(8, 8))
    plt.pie(taille_categorie_counts, labels=taille_categorie_counts.index, autopct='%1.1f%%', startangle=90, colors=['skyblue', 'lightcoral'])
    plt.title('Distribution of Taille Categorie (PME vs GE)')
    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.show()

    print("\nDistribution des valeurs dans 'taille_categorie':")
    print(tabulate(taille_categorie_counts.reset_index(), headers=['CatÃ©gorie de Taille', 'Count'], tablefmt='psql'))
else:
    print("'taille_categorie' column not found in the DataFrame.")

# prompt: i want to group 'Secteur industriel' selon :
# 'Agroalimentaire': 'Industrie lÃ©gÃ¨re',
#     'Automobile': 'Industrie lourde',
#     'AÃ©ronautique': 'Industrie lourde',
#     'Ã©lectrique/Ã©lectronique/Ã©lectromÃ©nager': 'Industrie lÃ©gÃ¨re',
#     'textile et habillement': 'Industrie lÃ©gÃ¨re',
#     'mÃ©canique et mÃ©tallurgie': 'Industrie lourde',
#     'Pharmaceutique': 'Industrie lourde',
#     'DÃ©veloppement embarquÃ©': 'Service et Technologie',
#     'Energitique': 'Service et Technologie',
#     'Consulting': 'Service et Technologie',
#     'Transport': 'Service et Technologie',
#     'cÃ¢blage': 'Industrie lÃ©gÃ¨re',
#     'Autres': 'Divers'

# Define the mapping
secteur_mapping = {
    'Agroalimentaire': 'Industrie lÃ©gÃ¨re',
    'Automobile': 'Industrie lourde',
    'AÃ©ronautique': 'Industrie lourde',
    'Ã©lectrique/Ã©lectronique/Ã©lectromÃ©nager': 'Industrie lÃ©gÃ¨re',
    'textile et habillement': 'Industrie lÃ©gÃ¨re',
    'mÃ©canique et mÃ©tallurgie': 'Industrie lourde',
    'Pharmaceutique': 'Industrie lourde',
    'DÃ©veloppement embarquÃ©': 'Service et Technologie',
    'Energitique': 'Service et Technologie',
    'Consulting': 'Service et Technologie',
    'Transport': 'Service et Technologie',
    'cÃ¢blage': 'Industrie lÃ©gÃ¨re',
    'Autres': 'Divers'
}

# Check if 'df' dataframe exists and is not empty
if 'df' in locals() and not df.empty:
  # Apply the mapping to create a new 'Secteur_Groupe' column
  if 'Secteur industriel' in df.columns:
    df['Secteur_Groupe'] = df['Secteur industriel'].map(secteur_mapping).fillna('Autre/Non dÃ©fini')
    # fillna handles any values in 'Secteur industriel' that are not in the mapping

    # Display the first few rows with the new column to verify
    print("\nDataFrame with new 'Secteur_Groupe' column:")
    print(df[['Secteur industriel', 'Secteur_Groupe']].head())

    # Display value counts for the new column
    print("\nValue counts for 'Secteur_Groupe':")
    print(df['Secteur_Groupe'].value_counts())

    # Optional: Check which values from 'Secteur industriel' were not in the mapping
    unmapped_sectors = df[df['Secteur_Groupe'] == 'Autre/Non dÃ©fini']['Secteur industriel'].unique()
    if len(unmapped_sectors) > 0:
        print("\nWarning: The following 'Secteur industriel' values were not in the mapping:")
        print(unmapped_sectors)

  else:
    print("'Secteur industriel' column not found in the DataFrame. Cannot group.")
else:
  print("DataFrame 'df' is not loaded or is empty. Cannot group sectors.")

# prompt: encode nominal ordinal column 'taille_categorie' et Nominal column 'Secteur_Groupe'

from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder

# Check if 'df' is loaded and required columns exist
if 'df' in locals() and not df.empty and 'taille_categorie' in df.columns and 'Secteur_Groupe' in df.columns:
  # --- Ordinal Encoding for 'taille_categorie' ---
  # Define the order for ordinal encoding
  # Assuming PME < GE is the desired order
  taille_order = [['PME', 'GE']]

  # Initialize OrdinalEncoder
  ordinal_encoder = OrdinalEncoder(categories=taille_order)

  # Reshape the column for the encoder (it expects a 2D array)
  df['taille_categorie_encoded'] = ordinal_encoder.fit_transform(df[['taille_categorie']])

  print("\nDataFrame with ordinal encoded 'taille_categorie':")
  print(df[['taille_categorie', 'taille_categorie_encoded']].head())
  print("\nValue counts for 'taille_categorie_encoded':")
  print(df['taille_categorie_encoded'].value_counts())


  # --- One-Hot Encoding for 'Secteur_Groupe' ---
  # Initialize OneHotEncoder
  # handle_unknown='ignore' will ignore categories not seen during fit
  # sparse_output=False returns a dense array
  onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)

  # Fit and transform the 'Secteur_Groupe' column
  secteur_encoded = onehot_encoder.fit_transform(df[['Secteur_Groupe']])

  # Create a DataFrame from the encoded data with appropriate column names
  secteur_encoded_df = pd.DataFrame(secteur_encoded, columns=onehot_encoder.get_feature_names_out(['Secteur_Groupe']))

  # Concatenate the original DataFrame with the new one-hot encoded columns
  # Ensure the indices match, especially if rows were dropped earlier
  df = pd.concat([df.reset_index(drop=True), secteur_encoded_df.reset_index(drop=True)], axis=1)

  print("\nDataFrame with one-hot encoded 'Secteur_Groupe':")
  print(df.head())

else:
  print("DataFrame 'df' is not loaded, is empty, or 'taille_categorie'/'Secteur_Groupe' columns not found.")

# prompt: export df

df.to_csv('processed_df.csv', index=False)
print("DataFrame exported successfully to 'processed_df.csv'")

features

# prompt: I want now to build a decision tree from df with 'cluster' as my target and remove columns in 'colonne'  and remove these columns 'Indicateurs suivis', 'Zone investissement principale',
#        'Typologie de production', 'Type de flux', 'Pays ' from features and use df in features

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix


target= 'Niveau MaturitÃ©'
# Identify columns that start with 'Secteur' or 'taille'
cols_to_drop_dynamic = [col for col in df.columns if col.startswith('Secteur') or col.startswith('taille')]
# Define the columns to remove from features
cols_to_remove_from_features = [
    'Indicateurs suivis',
    'Zone investissement principale',
    'Typologie de production',
    'Type de flux',
    'Pays ',
    'Cluster Label'
] + colonnes # Add the original survey question columns
# Add dynamic columns
cols_to_remove_from_features += cols_to_drop_dynamic
# Create the feature set by dropping the target and specified columns from df
# Make sure to drop the original multi-value columns ('MÃ©thodes Lean ', 'Technologies industrie 4.0') as they are replaced by cledf_cleaed_clustered
features = df.drop(columns=[target] + cols_to_remove_from_features + ['MÃ©thodes Lean ', 'Technologies industrie 4.0', 'cluster', 'Cluster', 'Feature_Cluster', 'Niveau MaturitÃ©'], errors='ignore')

# Ensure there are no remaining non-numeric columns that weren't intended to be features
# For robustness, drop any remaining object type columns if they exist unexpectedly
features = features.select_dtypes(exclude=['object'])

# Also, ensure there are no NaNs in the features or target
features.fillna(0, inplace=True) # Fill potential NaNs with 0 (common for dummy variables)
y = df[target].dropna()
features = features.loc[y.index] # Align features with the non-null target values


# Separate features (X) and target (y)
X = features
y = df[target]


# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=46, stratify=y) # Use stratify for balanced classes

# Initialize the Decision Tree Classifier
# You might want to tune hyperparameters like max_depth, min_samples_split, min_samples_leaf
dt_classifierAll = DecisionTreeClassifier(min_samples_leaf=2, random_state=10, max_depth=5)

# Train the classifier
dt_classifierAll.fit(X_train, y_train)

# Predict on the test set
y_pred = dt_classifierAll.predict(X_test)

# Evaluate the model
print("\n=== Decision Tree Model Evaluation ===")

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Optional: Feature Importances
print("\n=== Feature Importances ===")
feature_importances = pd.Series(dt_classifierAll.feature_importances_, index=X.columns)
print(feature_importances.sort_values(ascending=False))

# Visualize Feature Importances
plt.figure(figsize=(10, 8))
feature_importances.sort_values(ascending=False).head(20).plot(kind='barh') # Plot top 20
plt.title('Top 20 Feature Importances from Decision Tree')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

X_test

df.columns

# prompt: I want now to build a decision tree from df with 'cluster' as my target and remove columns in 'colonne'  and remove these columns 'Indicateurs suivis', 'Zone investissement principale',
#        'Typologie de production', 'Type de flux', 'Pays ' from features and use df in features

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

target = 'Niveau MaturitÃ©'

# Define the columns to remove from features
cols_to_remove_from_features = [
    'Indicateurs suivis',
    'Zone investissement principale',
    'Typologie de production',
    'Type de flux',
    'Pays ',
    'Cluster Label',
    'taille_categorie', 'Secteur_Groupe', 'taille_categorie_encoded',
    'Secteur_Groupe_Divers', 'Secteur_Groupe_Industrie lourde',
    'Secteur_Groupe_Industrie lÃ©gÃ¨re',
    'Secteur_Groupe_Service et Technologie'
] + colonnes # Add the original survey question columns
# Columns to remove based on prefix
cols_prefix_to_remove = [
    col for col in df.columns
    if col.startswith('Secteur') or col.startswith('taille')
]
cols_to_remove_from_features = (
    cols_to_remove_from_features + cols_prefix_to_remove
)
# Create the feature set by dropping the target and specified columns from df
# Make sure to drop the original multi-value columns ('MÃ©thodes Lean ', 'Technologies industrie 4.0') as they are replaced by cledf_cleaed_clustered
features = df.drop(columns=[target] + cols_to_remove_from_features + ['MÃ©thodes Lean ', 'Technologies industrie 4.0', 'cluster', 'Cluster', 'Feature_Cluster', 'Niveau MaturitÃ©'], errors='ignore')

# Ensure there are no remaining non-numeric columns that weren't intended to be features
# For robustness, drop any remaining object type columns if they exist unexpectedly
features = features.select_dtypes(exclude=['object'])

# Also, ensure there are no NaNs in the features or target
features.fillna(0, inplace=True) # Fill potential NaNs with 0 (common for dummy variables)
y = df[target].dropna()
features = features.loc[y.index] # Align features with the non-null target values

# Separate features (X) and target (y)
X = features
y = df[target]



# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=46, stratify=y) # Use stratify for balanced classes
# Initialize the Decision Tree Classifier
# You might want to tune hyperparameters like max_depth, min_samples_split, min_samples_leaf
dt_classifier = DecisionTreeClassifier(random_state=10, min_samples_leaf=2, max_depth=5)

# Train the classifier
dt_classifier.fit(X_train, y_train)

# Predict on the test set
y_pred = dt_classifier.predict(X_test)

print("Feature hash:")
print(hash(tuple(X.columns)))

print("X_test feature hash:")
print(hash(tuple(X_test.columns)))
# Evaluate the model
print("\n=== Decision Tree Model Evaluation ===")

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Optional: Visualize the Decision Tree (requires graphviz)
# !pip install graphviz
# from sklearn.tree import export_graphviz
# import graphviz

# # Export the tree to a DOT file
# dot_data = export_graphviz(dt_classifier,
#                             out_file=None,
#                             feature_names=X.columns,
#                             class_names=[str(c) for c in dt_classifier.classes_],
#                             filled=True, rounded=True,
#                             special_characters=True)

# # Create a graph from the DOT data
# graph = graphviz.Source(dot_data)

# # Render the graph (e.g., to a PNG file)
# # graph.render("decision_tree_clustered", view=True) # Uncomment to save and view the tree file

# # Display the graph directly in the notebook (may be large)
# # graph

# Optional: Feature Importances
print("\n=== Feature Importances ===")
feature_importances = pd.Series(dt_classifier.feature_importances_, index=X.columns)
print(feature_importances.sort_values(ascending=False))

# Visualize Feature Importances
plt.figure(figsize=(10, 8))
feature_importances.sort_values(ascending=False).head(20).plot(kind='barh') # Plot top 20
plt.title('Top 20 Feature Importances from Decision Tree')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

# prompt: i want to visualize the decision tree with graphviz

!pip install graphviz
from sklearn.tree import export_graphviz
import graphviz
X.columrens = X.columns.str.replace('&', 'and', regex=False)\
                     .str.replace('<', 'lt', regex=False)\
                     .str.replace('>', 'gt', regex=False)\
                     .str.replace('=', 'eq', regex=False)

# Export the tree to a DOT file
# Use the dt_classifier trained in the code snippet
dot_data = export_graphviz(dt_classifier,
                            out_file=None,
                            feature_names=X.columns, # Using X from the previous code for consistency
                            class_names=[str(c) for c in dt_classifier.classes_],
                            filled=True, rounded=True,
                            special_characters=True)

# Create a graph from the DOT data
graph = graphviz.Source(dot_data)

# Display the graph directly in the notebook
graph

print("Feature hash:")
print(hash(tuple(X.columns)))

print("X_test feature hash:")
print(hash(tuple(X_test.columns)))
# Evaluate the model
print("\n=== Decision Tree Model Evaluation ===")

import graphviz
from sklearn.tree import export_graphviz

# Ensure X.columns has valid names for graphviz
# X.columns was already modified in the previous cell to be graphviz-compatible

# Export the tree to a DOT file
dot_data = export_graphviz(dt_classifier,
                            out_file=None,
                            feature_names=X.columns,
                            class_names=[str(c) for c in dt_classifier.classes_],
                            filled=True, rounded=True,
                            special_characters=True)

# Create a graph from the DOT data
graph = graphviz.Source(dot_data)

# Render and save the graph to a file (e.g., PNG)
file_path = "decision_tree_classifier.png"
graph.render(file_path, format='png', view=False)
print(f"Decision tree image saved as {file_path}")

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Evaluate the dt_classifier model
y_pred_dt = dt_classifier.predict(X_test)

print("\n=== Decision Tree Classifier (Original) Model Evaluation ===")

# Accuracy
accuracy_dt = accuracy_score(y_test, y_pred_dt)
print(f"Accuracy: {accuracy_dt:.4f}")

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred_dt))

# Confusion Matrix
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_dt))